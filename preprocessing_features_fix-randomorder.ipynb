{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7a48b0-afab-48af-9b78-940d84045fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd6539a-045f-436d-a6fc-500b2cb45506",
   "metadata": {},
   "source": [
    "# 1) Loading the dataset\n",
    "Loading the dataset and removing columns that are not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f929854e-3af9-43e0-8236-34d299f3bf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ED visits dataset with all the features\n",
    "df_event_log_visits = pd.read_csv('df_visits_dur_timeofday-randomorder.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79207a37-f013-4afd-8f44-fcb6b7912f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_log_visits.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9193c5a2-6945-450a-a1b7-7c159cde8904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns that were added earlier for analytics\n",
    "df_event_log_visits.drop(['Earliest_Time'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742dcaf6-aa2f-48f4-be4b-2f5c18eda811",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_event_log_visits.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0467fd42-2ffd-4e58-a8c6-499e9cfec667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the Start_Time column to Activity_End_Time - there is only one time stamp in the dataset which is assumed to be\n",
    "# when the activity finished\n",
    "df_event_log_visits.rename(columns={'Start_Time': 'Activity_End_Time'}, inplace=True)\n",
    "\n",
    "# Drop the End_Time column\n",
    "df_event_log_visits.drop(columns=['End_Time'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e0cb62-4c79-4a3b-85da-c29cdde4eb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sort the DataFrame by VISIT_ID and Activity_End_Time\n",
    "df_event_log_visits.sort_values(by=['VISIT_ID', 'Activity_End_Time'], inplace=True)\n",
    "\n",
    "# Infer Start_Time for each activity using the Activity_End_Time of the previous activity\n",
    "df_event_log_visits['Start_Time'] = df_event_log_visits['Activity_End_Time'].shift(1)\n",
    "\n",
    "# Ensure that the Start_Time is not carried over from the last activity of the previous VISIT_ID to the first activity of the next VISIT_ID\n",
    "df_event_log_visits.loc[df_event_log_visits['VISIT_ID'] != df_event_log_visits['VISIT_ID'].shift(1), 'Start_Time'] = pd.NaT\n",
    "\n",
    "# For the first activity in each sequence, set Start_Time equal to Activity_End_Time, indicating zero duration\n",
    "df_event_log_visits.loc[df_event_log_visits['Start_Time'].isna(), 'Start_Time'] = df_event_log_visits['Activity_End_Time']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d2079c-925f-493a-9c66-177b1df03880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the remaining columns in the dataset as of now\n",
    "df_event_log_visits.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53959dc-7402-4bf9-a5cf-0b78dda621da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the 'Duration' column to timedelta\n",
    "df_event_log_visits['Duration'] = pd.to_timedelta(df_event_log_visits['Duration'])\n",
    "\n",
    "# Rename the column and convert the values to hours\n",
    "df_event_log_visits['Duration_hours'] = df_event_log_visits['Duration'].dt.total_seconds() / 3600\n",
    "\n",
    "# Drop the old 'Duration' column\n",
    "df_event_log_visits = df_event_log_visits.drop(columns=['Duration'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eee3cf-823c-4663-b5f4-fa5676bb25d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# filtering out the columns I move to the end\n",
    "columns = list(df_event_log_visits.columns)\n",
    "columns.remove('Activity_End_Time')\n",
    "columns.remove('Duration_hours')\n",
    "\n",
    "\n",
    "# Appending them\n",
    "columns = columns + ['Activity_End_Time', 'Duration_hours']\n",
    "\n",
    "# Reordring based on the new column order\n",
    "df_event_log_visits = df_event_log_visits[columns]\n",
    "\n",
    "df_event_log_visits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007091cd-f2e7-4878-a869-420838c7c6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_log_visits.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb61aae3-2939-4e27-9612-b89ad5959960",
   "metadata": {},
   "source": [
    "# 2) Dealing with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adea4639-2a01-464c-b265-e08d8e687763",
   "metadata": {},
   "source": [
    "## DOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d2039c-65b2-48d8-962d-bc7d9111060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOB is the first column with missing values - there are in total for rows 4 missing values - these rows can be droppped\n",
    "\n",
    "#  Removing records with missing DOB\n",
    "df_event_log_visits = df_event_log_visits.dropna(subset=['DOB'])\n",
    "\n",
    "df_event_log_visits.info()\n",
    "\n",
    "\n",
    "\n",
    "# Next is DOD, however the missing value in this case indicates that the patient is still alive, hence getting rid of these values \n",
    "# would lead to losing important information - this column will be dealt with in later step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff07dc3-9bdb-4e1d-9ee6-b9f88035e6d7",
   "metadata": {},
   "source": [
    "## POSTAL_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b67ef09-1fd0-4e4a-9d73-9467525966e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another column with missing values is the postal code, in this case I decided to fill in the values based on the value of the hospital\n",
    "# that the patient visited\n",
    "\n",
    "# Filtering the df to only include rows where ED_POSTAL_CODE is missing\n",
    "missing_postal_code = df_event_log_visits[df_event_log_visits['ED_POSTAL_CODE'].isnull()]\n",
    "\n",
    "# Printing the rows with missing ED_POSTAL_CODE\n",
    "print(missing_postal_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9318d2a9-8663-4688-84cc-c0a79e7ceed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting occurrences of each FACILITY_ID in the DataFrame with missing postal codes\n",
    "facility_counts = missing_postal_code['FACILITY_ID'].value_counts()\n",
    "\n",
    "# Printing the counts\n",
    "print(facility_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94daf72-26ee-4505-92d5-3f6d723da083",
   "metadata": {},
   "outputs": [],
   "source": [
    "facility_postal_map = {\n",
    "    'HSC': 'A1B 3V6',\n",
    "    'GBC': 'A5A 1K3',\n",
    "    'SCM': 'A1C 5B8',\n",
    "    'BUR': 'A0E 1E0',\n",
    "    'CGH': 'A1Y 1A4'\n",
    "}\n",
    "\n",
    "# Applying the map to the 'FACILITY_ID' column to create a new 'Imputed_Postal_Code' column\n",
    "df_event_log_visits['Imputed_Postal_Code'] = df_event_log_visits['FACILITY_ID'].map(facility_postal_map)\n",
    "\n",
    "# Filling missing 'ED_POSTAL_CODE' values with the imputed values from 'Imputed_Postal_Code'\n",
    "df_event_log_visits['ED_POSTAL_CODE'].fillna(df_event_log_visits['Imputed_Postal_Code'], inplace=True)\n",
    "\n",
    "# dropping the column\n",
    "df_event_log_visits.drop('Imputed_Postal_Code', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b434f8-4ade-431a-9070-cd68ac3d0c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if there are any remaining missing values in 'ED_POSTAL_CODE'\n",
    "print(df_event_log_visits['ED_POSTAL_CODE'].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86764ba-2010-43f0-bbf8-183cfb00bec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_log_visits.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93653af8-9d6d-46c7-8b2c-05b9ad17b7ce",
   "metadata": {},
   "source": [
    "## CTAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea872308-05fb-4bc8-a787-c5ad17d9ee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next missing values are in the case of CTAS column \n",
    "\n",
    "# Histogram to see the distribution of CTAS scores\n",
    "df_event_log_visits['CTAS'].hist(bins=5)\n",
    "plt.title('Distribution of CTAS Scores')\n",
    "plt.xlabel('CTAS Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Descriptive statistics\n",
    "print(df_event_log_visits['CTAS'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30f94d7-1fdb-4c74-a940-47b8e69de65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for rows where CTAS is missing\n",
    "missing_ctas_df = df_event_log_visits[df_event_log_visits['CTAS'].isna()]\n",
    "\n",
    "# Display how many of these have 'TLWBS' or 'RLWBS' as the depart_disposition_id\n",
    "missing_ctas_dispositions = missing_ctas_df['DEPART_DISPOSITION_ID'].value_counts()\n",
    "print(missing_ctas_dispositions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda3979d-4fab-4694-b075-a0cb8198d0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the total counts of each DEPART_DISPOSITION_ID in the whole dataset\n",
    "total_dispositions = df_event_log_visits['DEPART_DISPOSITION_ID'].value_counts()\n",
    "\n",
    "# Calculate proportions for the missing CTAS group\n",
    "missing_ctas_proportions = missing_ctas_dispositions / missing_ctas_dispositions.sum()\n",
    "\n",
    "# Calculate proportions for the whole dataset\n",
    "total_proportions = total_dispositions / total_dispositions.sum()\n",
    "\n",
    "# Create a DataFrame for easy comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Missing_CTAS_Proportion': missing_ctas_proportions,\n",
    "    'Total_Proportion': total_proportions\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(comparison_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e995003-8b03-4f90-9630-4e5231aad3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A significant proportion of 'RLWBS' (25.02%) and 'TLWBS' (6.41%) is in the missing CTAS dataset, this is higher compared\n",
    "# to their proportions in the entire dataset (0.26% and 10.80% respectively). I will code it “9”.\n",
    "\n",
    "# Coding missing CTAS values as 9\n",
    "df_event_log_visits['CTAS'].fillna(9, inplace=True)\n",
    "\n",
    "\n",
    "# Verifying no missing values\n",
    "print(f\"After imputation, remaining missing values in 'CTAS': {df_event_log_visits['CTAS'].isnull().sum()}\")\n",
    "\n",
    "# the distribution after imputation\n",
    "df_event_log_visits['CTAS'].hist(bins=5)\n",
    "plt.title('Distribution of CTAS Scores After Imputation')\n",
    "plt.xlabel('CTAS Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093b6652-b97a-42b9-a383-93ce0316bd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_log_visits.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6a95d8-f016-433a-9810-81ed970e139b",
   "metadata": {},
   "source": [
    "## REASON_FOR_VISIT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fda8c65-1fb4-4c9c-9abc-f683073083c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_log_visits['REASON_FOR_VISIT'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76477e7e-8680-4130-989b-6f9deff5c615",
   "metadata": {},
   "outputs": [],
   "source": [
    "## too many values - decided to drop the column  \n",
    "df_event_log_visits.drop(columns=['REASON_FOR_VISIT'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198a501c-1f58-4eee-89c5-51af9e190088",
   "metadata": {},
   "source": [
    "## PRESENTING_COMPLAINT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ac5b24-6f40-443d-8fef-2b827c9a2f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_log_visits['PRESENTING_COMPLAINT'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a011a6c7-9f0b-48c5-8387-7a9a1de7ae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in the PRESENTING_COMPLAINT column\n",
    "df_event_log_visits.dropna(subset=['PRESENTING_COMPLAINT'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3b4466-3273-4875-b520-132a957f2022",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_log_visits.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ff0385-77e9-4f71-a566-e57ec4641ee3",
   "metadata": {},
   "source": [
    "## DEPART_DISPOSITION_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348a4e58-cc0c-482d-90be-53fcbbceb197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Departing disposition is one of the values I will be predicting with the model \n",
    "# (TRIAGED LEFT W/O BEING SEEN and REGISTERED LEFT W/O BEING SEEN)\n",
    "# hence I decided to not deal with the missing values at this moment - however, we only need one of the columns\n",
    "df_event_log_visits['DEPART_DISPOSITION_DESC'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91a40cd-af0d-4646-ab83-29771232bc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_log_visits.drop(['DEPART_DISPOSITION_DESC'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46cc78c-57cb-4d4f-b679-b6a59c37648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dispositions_counts =  df_event_log_visits['DEPART_DISPOSITION_ID'].value_counts()\n",
    "\n",
    "# Print the counts\n",
    "print(dispositions_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d8d451-a8c0-4b3e-97df-a477f3631005",
   "metadata": {},
   "source": [
    "# 3) Preprocessing columns\n",
    "In the next steps I am preprocessing and cleaning data in all columns that will be used by the deep learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa185df1-fddf-445a-907e-d19428c973d8",
   "metadata": {},
   "source": [
    "## SEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5f9f76-63f5-48d5-bbb0-0b7c6ccc3442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the SEX column\n",
    "\n",
    "df_event_log_visits['SEX'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d01a1-1edc-45a2-8fcc-375f2f9c5c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode 'M' as 0 and 'F' as 1 and 'U' as 2\n",
    "df_event_log_visits['SEX'] = df_event_log_visits['SEX'].map({'M': 0, 'F': 1, 'U': 2})\n",
    "df_event_log_visits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b828fc0e-d9b4-42c2-b5d6-c19b0324c9ff",
   "metadata": {},
   "source": [
    "## DATE OF BIRTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e6d094-5c27-46a2-afee-b18753626d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the DOB column\n",
    "\n",
    "# converting the 'DOB' column to datetime\n",
    "df_event_log_visits['DOB'] = pd.to_datetime(df_event_log_visits['DOB'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ffde08-963f-49f2-bf90-3689d8afa61e",
   "metadata": {},
   "source": [
    "## PRESENTING_COMPLAINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43f6af1-61bf-4524-9a65-d7f60430d448",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Apply label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df_event_log_visits['PRESENTING_COMPLAINT_ENCODED'] = label_encoder.fit_transform(df_event_log_visits['PRESENTING_COMPLAINT'])\n",
    "\n",
    "# Drop the original PRESENTING_COMPLAINT column\n",
    "df_event_log_visits.drop(columns=['PRESENTING_COMPLAINT'], inplace=True)\n",
    "\n",
    "df_event_log_visits.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40113843-ad17-4f99-ba2d-15c2e5006d62",
   "metadata": {},
   "source": [
    "## 3.1) Creating new features based on the old features\n",
    "Here, I am creating new features based on the available features in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb4aef6-3f14-4ab9-bb45-ae40c7811970",
   "metadata": {},
   "source": [
    "## DATE OF DEATH and IS_DECEASED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ef63c3-58c3-46f6-b5e6-5e566f47814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the DOD column by creating a binary indicator from DOD\n",
    "df_event_log_visits['Is_Deceased'] = df_event_log_visits['DOD'].notnull().astype(int)\n",
    "\n",
    "df_event_log_visits['DOD'] = pd.to_datetime(df_event_log_visits['DOD'], errors='coerce')\n",
    "\n",
    "df_event_log_visits.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfdf5fa-8bdd-4283-8ee7-3a6cb10b7fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# inspecting the unique values in 'DEPART_DISPOSITION_ID' when 'Is_Deceased' is True\n",
    "deceased_dispositions = df_event_log_visits[df_event_log_visits['Is_Deceased'] == 1]['DEPART_DISPOSITION_ID'].value_counts()\n",
    "\n",
    "# A cross-tabulation to see the relationship more clearly\n",
    "crosstab = pd.crosstab(df_event_log_visits['Is_Deceased'], df_event_log_visits['DEPART_DISPOSITION_ID'])\n",
    "\n",
    "print(\"Distribution of departure dispositions for deceased cases:\")\n",
    "print(deceased_dispositions)\n",
    "\n",
    "print(\"\\nCross-tabulation of 'Is_Deceased' and 'DEPART_DISPOSITION_ID':\")\n",
    "print(crosstab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01edac6-7214-4d5b-9a36-b75f580e2664",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_event_log_visits['DOD'] = pd.to_datetime(df_event_log_visits['DOD'], errors='coerce').dt.tz_localize(None).dt.tz_localize('UTC')\n",
    "\n",
    "# comparisons\n",
    "df_event_log_visits['Died_Before_Visit'] = (df_event_log_visits['DOD'] < df_event_log_visits['Start_Time']).astype(int)\n",
    "df_event_log_visits['Died_During_Visit'] = ((df_event_log_visits['DOD'] >= df_event_log_visits['Start_Time']) & \n",
    "                                            (df_event_log_visits['DOD'] <= df_event_log_visits['Activity_End_Time'])).astype(int)\n",
    "df_event_log_visits['Died_After_Visit'] = (df_event_log_visits['DOD'] > df_event_log_visits['Activity_End_Time']).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa039865-7db3-4e19-89f0-dfe9f1c3abb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for rows where 'Is_Deceased' is 1 and display the first few rows\n",
    "deceased_visits = df_event_log_visits[df_event_log_visits['Is_Deceased'] == 1]\n",
    "deceased_visits.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ffa720-bd22-443c-bdbe-c020d43fbcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking counts:\n",
    "died_before_visit_count = df_event_log_visits[df_event_log_visits['Died_Before_Visit'] == 1].shape[0]\n",
    "print(\"Number of cases where death occurred before the visit:\", died_before_visit_count)\n",
    "\n",
    "died_during_visit_count = df_event_log_visits[df_event_log_visits['Died_During_Visit'] == 1].shape[0]\n",
    "print(\"Number of cases where death occurred during the visit:\", died_during_visit_count)\n",
    "\n",
    "died_after_visit_count = df_event_log_visits[df_event_log_visits['Died_After_Visit'] == 1].shape[0]\n",
    "print(\"Number of cases where death occurred after the visit:\", died_after_visit_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028bc85d-8097-4ecf-97e8-aeab3c61388c",
   "metadata": {},
   "source": [
    "## AREA_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ec8d08-5180-4a0d-b13d-ebf418a80bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifying rural or urban area based on the postal code according to\n",
    "# https://www.canadapost-postescanada.ca/cpc/en/support/articles/addressing-guidelines/postal-codes.page\n",
    "\n",
    "# Function to determine if a postal code is urban or rural\n",
    "def urban_rural_classifier(postal_code):\n",
    "    # Check the second character of the postal code\n",
    "    if postal_code[1] == '0':\n",
    "        return 0  # Rural\n",
    "    else:\n",
    "        return 1  # Urban\n",
    "\n",
    "# Apply the function to classify each postal code\n",
    "df_event_log_visits['Area_Type'] = df_event_log_visits['ED_POSTAL_CODE'].apply(urban_rural_classifier)\n",
    "\n",
    "# Verify by displaying the DataFrame\n",
    "print(df_event_log_visits[['ED_POSTAL_CODE', 'Area_Type']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67df2955-bbfa-4d46-b3ce-9f8a3a060967",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_log_visits.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d618c59-1af0-41d7-9ca9-cd8a4dd0b1cd",
   "metadata": {},
   "source": [
    "## COORDINATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579a2b5e-d9b5-440c-98cb-ede6a242e91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding latutide and longitude table from geonames\n",
    "file_path = 'CA_full.txt'\n",
    "postal_codes = pd.read_csv(file_path, delimiter='\\t', names=[\n",
    "    'Country', 'Postal_Code', 'Place_Name', 'Province', 'Province_Code',\n",
    "    'Unnamed5', 'Unnamed6', 'Unnamed7', 'Unnamed8', 'Latitude', 'Longitude', 'Extra'\n",
    "], skiprows=1)  # skiprows=1 to skip the example header row you pasted\n",
    "\n",
    "# Dropping the unnecessary columns\n",
    "postal_codes.drop(columns=['Unnamed5', 'Unnamed6', 'Unnamed7', 'Unnamed8', 'Extra'], inplace=True)\n",
    "\n",
    "# Displaying the dataframe to ensure it loaded correctly\n",
    "print(postal_codes.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d07d3af-2ce7-44f0-b5db-56b6ecaa193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for duplicates \n",
    "\n",
    "print(postal_codes['Postal_Code'].duplicated().sum())\n",
    "postal_codes = postal_codes.drop_duplicates(subset=['Postal_Code'], keep='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020e2d9e-3947-4a11-b6b6-8a81b852b6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the dataframes\n",
    "df_event_log_visits = df_event_log_visits.merge(postal_codes, left_on='ED_POSTAL_CODE', right_on='Postal_Code', how='left')\n",
    "\n",
    "# Checking for missing latitude and longitude after the merge\n",
    "print(df_event_log_visits[['Latitude', 'Longitude']].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b778a7-33bf-4723-8265-3eed62650477",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_log_visits.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb13cfbc-2334-40db-a67b-e0d199379cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some of the postal codes can not be found on geonames as they are - the closest coordinates can be found by using\n",
    "# only the first three digits\n",
    "postal_codes['First_3_Digits'] = postal_codes['Postal_Code'].str[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620f6930-01ea-4354-b174-0358b2b15794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows where 'Latitude' or 'Longitude' is missing \n",
    "df_event_log_visits_missing_lat = df_event_log_visits[df_event_log_visits['Latitude'].isnull() | df_event_log_visits['Longitude'].isnull()]\n",
    "\n",
    "# creating nnew column\n",
    "df_event_log_visits_missing_lat['First_3_Digits'] = df_event_log_visits['ED_POSTAL_CODE'].str[:3]\n",
    "df_event_log_visits_missing_lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ffbef2-e05d-4c78-84ef-3d1fd09d39fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optimize data types\n",
    "df_event_log_visits_missing_lat['First_3_Digits'] = df_event_log_visits_missing_lat['First_3_Digits'].astype('category')\n",
    "postal_codes['First_3_Digits'] = postal_codes['First_3_Digits'].astype('category')\n",
    "\n",
    "# Select only relevant columns for the merge\n",
    "df_event_log_visits_missing_lat = df_event_log_visits_missing_lat[['First_3_Digits']]\n",
    "postal_codes = postal_codes[['First_3_Digits', 'Latitude', 'Longitude']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7984a269-73ba-4f2b-8098-26cbc0c4f20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the merge\n",
    "df_merged = df_event_log_visits_missing_lat.merge(postal_codes, on='First_3_Digits', how='left', suffixes=('', '_pc'))\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(df_merged.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d677430-6248-4f56-a5a5-43f32ef61b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding new column to the original dataset to merge on\n",
    "df_event_log_visits['First_3_Digits'] = df_event_log_visits['ED_POSTAL_CODE'].str[:3]\n",
    "df_event_log_visits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afd1812-fe58-4b70-b0e2-61ab7d172444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the rows in the main dataset where latitude and longitude are missing\n",
    "missing_indices = df_event_log_visits[df_event_log_visits['Latitude'].isna() & df_event_log_visits['Longitude'].isna()].index\n",
    "\n",
    "# Update the main dataset with the latitude and longitude from the merged DataFrame\n",
    "df_event_log_visits.loc[missing_indices, 'Latitude'] = df_merged['Latitude']\n",
    "df_event_log_visits.loc[missing_indices, 'Longitude'] = df_merged['Longitude']\n",
    "\n",
    "# Check if the missing values are updated\n",
    "print(df_event_log_visits.loc[missing_indices, ['First_3_Digits', 'Latitude', 'Longitude']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdcd34b-0ba2-4da4-b406-d2fdcd9dd09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an overview of all missing values in the DataFrame\n",
    "print(df_event_log_visits.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f121a6-bc3c-424b-9b38-adc3bcd7a959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows where either Latitude or Longitude is missing (3 rows)\n",
    "df_event_log_visits = df_event_log_visits.dropna(subset=['Latitude', 'Longitude'])\n",
    "\n",
    "# Verifying the result\n",
    "print(df_event_log_visits[['Latitude', 'Longitude']].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57db860-e221-4092-8c5e-d673a0818947",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming 'First_3_Digits' represents the first 3 digits of postal codes\n",
    "\n",
    "# List of postal codes corresponding to Conception Bay ('A1X', 'A1W' - CBS , 'A0A', 'A1Y' - CBN)\n",
    "conception_bay_postal_codes = ['A1X', 'A1W', 'A0A', 'A1Y']\n",
    "\n",
    "# Filter the dataset for Conception Bay postal codes\n",
    "cbn_data = df_event_log_visits[df_event_log_visits['First_3_Digits'].isin(conception_bay_postal_codes)]\n",
    "\n",
    "# Create a cross-tabulation of postal codes with hospital sites\n",
    "cross_tab = pd.crosstab(cbn_data['First_3_Digits'], cbn_data['FACILITY_NAME'])\n",
    "\n",
    "# Output the cross-tabulation\n",
    "cross_tab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd26fc9d-fddb-42ec-8eeb-05b2dee8e7a5",
   "metadata": {},
   "source": [
    "## DISTANCE_TO_HOSPITAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e0d29b-7cc0-48fd-bfa8-8e2ecdd500aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic\n",
    "\n",
    "# Hospital coordinates dictionary (includes all hospitals)\n",
    "hospital_coords = {\n",
    "    'Health Sciences Centre - St. Johns': (47.57173844188426, -52.7428752808959),\n",
    "    'St. Clares Mercy Hospital - St. Johns': (47.55783035337661, -52.72164112844875),\n",
    "    'Dr. G.B. Cross Memorial Hospital - Clarenville': (48.165186482963676, -53.98449946306605),\n",
    "    'Carbonear General Hospital - Carbonear': (47.725414948091, -53.226560474732224),\n",
    "    'Burin Peninsula Health care Centre - Burin': (47.10500591911938, -55.19465935942642),  \n",
    "}\n",
    "\n",
    "# Calculating the distance between a case and a hospital\n",
    "def calculate_distance(row, hospital_name):\n",
    "    case_coords = (row['Latitude'], row['Longitude'])\n",
    "    hospital_coords_tuple = hospital_coords.get(hospital_name, None)  \n",
    "    if hospital_coords_tuple is None:\n",
    "        return float('nan')  # Return NaN if the hospital isn't in the dictionary\n",
    "    return geodesic(case_coords, hospital_coords_tuple).km  # Distance in kilometers\n",
    "\n",
    "# Applying the function to calculate distances for all hospitals\n",
    "df_event_log_visits['Distance_to_Hospital'] = df_event_log_visits.apply(\n",
    "    lambda row: calculate_distance(row, row['FACILITY_NAME']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Displaying DataFrame with the calculated distances\n",
    "print(df_event_log_visits[['VISIT_ID', 'FACILITY_NAME', 'Distance_to_Hospital']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65060935-5d5e-4865-b409-f64d8bd7861f",
   "metadata": {},
   "source": [
    "## IS_NL_HOLIDAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbdcf51-50a4-407e-9134-ea071eecf7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Newfoundland and Labrador holidays in \"MM-DD\" format\n",
    "nl_holidays = {\n",
    "    \"01-01\",  # New Year's Day\n",
    "    \"03-17\",  # St. Patrick's Day\n",
    "    \"04-23\",  # St. George's Day\n",
    "    \"06-24\",  # Discovery Day\n",
    "    \"07-01\",  # Canada Day\n",
    "    \"09-02\",  # Labour Day\n",
    "    \"10-14\",  # Thanksgiving\n",
    "    \"11-11\",  # Remembrance Day\n",
    "    \"12-25\",  # Christmas Day\n",
    "    \"12-26\"   # Boxing Day\n",
    "}\n",
    "\n",
    "# checking if a given date matches a holiday\n",
    "def is_nl_holiday(date):\n",
    "    # Extracting month and day from the date\n",
    "    month_day = date.strftime(\"%m-%d\")\n",
    "    return month_day in nl_holidays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04073a45-a3ed-4c2e-a47d-48df44914677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting 'Start_Time' to datetime \n",
    "df_event_log_visits['Start_Time'] = pd.to_datetime(df_event_log_visits['Start_Time'])\n",
    "\n",
    "# Applying the function to create a new column\n",
    "df_event_log_visits['Is_NL_Holiday'] = df_event_log_visits['Start_Time'].apply(is_nl_holiday)\n",
    "\n",
    "# Displaying results\n",
    "print(df_event_log_visits[['Start_Time', 'Is_NL_Holiday']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53124b45-8b75-447f-b8a0-1d190c55c832",
   "metadata": {},
   "source": [
    "## DAY_OF_WEEK and IS_WEEKEND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6515268f-9b99-4b8e-b731-e4c1b0911168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the day of the week (0 = Monday, 6 = Sunday)\n",
    "df_event_log_visits['Day_of_Week'] = pd.to_datetime(df_event_log_visits['Start_Time']).dt.dayofweek\n",
    "df_event_log_visits['Is_Weekend'] = df_event_log_visits['Day_of_Week'].isin([5, 6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e882e07b-ee44-4215-a18c-c019ecd333c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_log_visits.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb084a0a-3742-4444-bbfe-bbee7776e47a",
   "metadata": {},
   "source": [
    "## VISIT_SEASON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ecd18a-090e-4075-afa3-b78b76f17d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining season ranges\n",
    "seasons = {\n",
    "    \"Winter\": (1, 2, 12),\n",
    "    \"Spring\": (3, 4, 5),\n",
    "    \"Summer\": (6, 7, 8),\n",
    "    \"Fall\": (9, 10, 11)\n",
    "}\n",
    "\n",
    "# Function to get the season\n",
    "def get_season(month):\n",
    "    for season, months in seasons.items():\n",
    "        if month in months:\n",
    "            return season\n",
    "    return None\n",
    "\n",
    "df_event_log_visits['Visit_Season'] = df_event_log_visits['Start_Time'].apply(\n",
    "    lambda x: get_season(pd.to_datetime(x).month)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0188ea-117f-409c-9dde-165e41ce77de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_log_visits.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44f3e7e-8152-4134-a3d0-23039296904e",
   "metadata": {},
   "source": [
    "## VISIT_FREQUENCY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57f52ef-7c8d-4baa-a362-07afbe8a0021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring 'Start_Time' is in datetime format\n",
    "df_event_log_visits['Start_Time'] = pd.to_datetime(df_event_log_visits['Start_Time'])\n",
    "\n",
    "# Sorting the data\n",
    "df_event_log_visits.sort_values(by=['SID', 'Start_Time'], inplace=True)\n",
    "\n",
    "# Identifying the latest visit for each SID\n",
    "df_event_log_visits['Latest_Visit'] = df_event_log_visits.groupby('SID')['Start_Time'].transform('max')\n",
    "\n",
    "# Marking rows that correspond to the latest visit\n",
    "df_event_log_visits['Is_Latest_Visit'] = df_event_log_visits['Start_Time'] == df_event_log_visits['Latest_Visit']\n",
    "\n",
    "# Counting all unique visits per SID\n",
    "visit_counts = df_event_log_visits.groupby('SID')['VISIT_ID'].nunique()\n",
    "\n",
    "# Subtracting 1 from the counts to exclude the current visit (where there are previous visits)\n",
    "visit_counts = visit_counts - 1\n",
    "\n",
    "# Ensuring that no negative counts are present (in cases with only one visit)\n",
    "visit_counts = visit_counts.clip(lower=0)\n",
    "\n",
    "# Mapping the adjusted visit counts back to the original DataFrame\n",
    "df_event_log_visits['Visit_Frequency'] = df_event_log_visits['SID'].map(visit_counts)\n",
    "\n",
    "# Display the result\n",
    "print(df_event_log_visits[['SID', 'VISIT_ID', 'Visit_Frequency']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03025fea-0caa-476f-bdb9-c90f342d4b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_log_visits.drop(columns=['Latest_Visit', 'Is_Latest_Visit'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63710ca0-7d58-45e2-bae2-895e3ecabde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_log_visits.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3a677e-5879-4ef7-b711-20e3b957c216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'TLWBS' and 'RLWBS' codes indicate left without being seen\n",
    "lwbs_codes = ['TLWBS', 'RLWBS']\n",
    "df_event_log_visits['Is_LWBS'] = df_event_log_visits['DEPART_DISPOSITION_ID'].isin(lwbs_codes).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e5dbf9-6fb7-4302-9a0f-f82b922cb551",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_event_log_visits['Start_Time'] = pd.to_datetime(df_event_log_visits['Start_Time'])\n",
    "\n",
    "# Sorting the DataFrame by 'SID' and 'Start_Time'\n",
    "df_event_log_visits.sort_values(by=['SID', 'Start_Time'], inplace=True)\n",
    "\n",
    "# Identifying the last visit for each SID\n",
    "df_event_log_visits['Latest_Visit'] = df_event_log_visits.groupby('SID')['Start_Time'].transform('max')\n",
    "\n",
    "# Excluding the latest visit from counting\n",
    "condition = df_event_log_visits['Start_Time'] < df_event_log_visits['Latest_Visit']\n",
    "df_prior_visits = df_event_log_visits[condition]\n",
    "\n",
    "# Summing up LWBS occurrences per visit, excluding the latest visit\n",
    "lwbs_per_visit = df_prior_visits.groupby(['SID', 'VISIT_ID'])['Is_LWBS'].max().groupby('SID').cumsum().reset_index()\n",
    "\n",
    "# Renaming the column in lwbs_per_visit before merging\n",
    "lwbs_per_visit.rename(columns={'Is_LWBS': 'Prior_LWBS'}, inplace=True)\n",
    "\n",
    "# Merging this cumulative count back to the original DataFrame on both SID and VISIT_ID\n",
    "df_event_log_visits = df_event_log_visits.merge(lwbs_per_visit, on=['SID', 'VISIT_ID'], how='left', suffixes=('', '_cumulative'))\n",
    "\n",
    "# Replacing NaN with 0 for visits without any prior LWBS events\n",
    "df_event_log_visits['Prior_LWBS'].fillna(0, inplace=True)\n",
    "\n",
    "# Dropping the temporary column if no longer needed\n",
    "df_event_log_visits.drop(columns='Latest_Visit', inplace=True)\n",
    "\n",
    "# Display the DataFrame to verify results\n",
    "print(df_event_log_visits[['SID', 'VISIT_ID', 'Prior_LWBS']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17641d52-c17d-4485-b696-4624df2914e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering rows for SID 'STDY111131' and displaying specific columns\n",
    "filtered_rows = df_event_log_visits[df_event_log_visits['SID'] == 'STDY111131']\n",
    "print(filtered_rows[['SID', 'VISIT_ID', 'Prior_LWBS', 'DEPART_DISPOSITION_ID']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a027d13e-08c3-4e1e-ad03-9b4446bb9e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_log_visits.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72ebac1-45f7-4fb4-bb59-cf056a2a47e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_log_visits.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f30d87-3055-4536-b806-c92eb6de3a18",
   "metadata": {},
   "source": [
    "## ED_BUSINESS_HOURLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39257ad-a708-4350-869c-df9a36ecfc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Groupping by hour to get the number of patient arrivals in each hour\n",
    "arrival_counts = df_event_log_visits.groupby(df_event_log_visits['Start_Time'].dt.floor('H')).size()\n",
    "\n",
    "# Mappinng the hourly counts back to the original DataFrame\n",
    "df_event_log_visits['ED_Business_Hourly'] = df_event_log_visits['Start_Time'].dt.floor('H').map(arrival_counts)\n",
    "\n",
    "\n",
    "print(df_event_log_visits[['Start_Time', 'ED_Business_Hourly']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4371e5-d69c-4b26-a747-6c6fdf0ad940",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_event_log_visits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6555cb0f-213e-4714-b7cc-a65cdb908ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_log_visits.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fd05d9-1f03-4164-94a9-210af3b28ee4",
   "metadata": {},
   "source": [
    "##  The acuity score for each hour for each site "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf73917-cbf5-45e5-85c6-8ece2ebe3831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Start_Time and Activity_End_Time to datetime \n",
    "df_event_log_visits['Start_Time'] = pd.to_datetime(df_event_log_visits['Start_Time'])\n",
    "df_event_log_visits['Activity_End_Time'] = pd.to_datetime(df_event_log_visits['Activity_End_Time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06b0c51-abe0-418b-aa02-95c1dedf4148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Inverse_CTAS\n",
    "df_event_log_visits['Inverse_CTAS'] = 1 / df_event_log_visits['CTAS']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c745ea7-95aa-4921-8af8-fa34430b3d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize an empty DataFrame to store acuity scores\n",
    "acuity_scores = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a908c0-0465-4e49-92cb-1a2fe4acd4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Iterate through each facility\n",
    "for facility in df_event_log_visits['FACILITY_ID'].unique():\n",
    "    # Filter data for the current facility\n",
    "    facility_data = df_event_log_visits[df_event_log_visits['FACILITY_ID'] == facility]\n",
    "    \n",
    "     # Create an hourly index for the time span covered by the facility's data\n",
    "    # hourly_index: This is a range of timestamps, \n",
    "    # starting from start_time and ending at end_time, with a frequency of 1 hour (freq='H').\n",
    "    # This hourly index includes both the date and time components for each hour in the specified range.\n",
    "\n",
    "    start_time = facility_data['Start_Time'].min()\n",
    "    end_time = facility_data['Activity_End_Time'].max()\n",
    "    hourly_index = pd.date_range(start=start_time, end=end_time, freq='H')\n",
    "    \n",
    "    # Initialize a list to hold the active cases per hour\n",
    "    hourly_acuity = []\n",
    "    \n",
    "    # Determine active cases for each hour\n",
    "    for hour in hourly_index:\n",
    "        # Filter the data to get active cases at the current hour - A case is considered active if the Start_Time is before or at the hour,\n",
    "        # and the Activity_End_Time is after or at the hour.\n",
    "        active_cases = facility_data[(facility_data['Start_Time'] <= hour) & \n",
    "                                     (facility_data['Activity_End_Time'] >= hour)]\n",
    "        # Calculate the mean and std deviation of Inverse_CTAS for active cases\n",
    "        if not active_cases.empty:\n",
    "            mean_inverse_ctas = active_cases['Inverse_CTAS'].mean()\n",
    "            std_inverse_ctas = active_cases['Inverse_CTAS'].std()\n",
    "            mean_age = active_cases['VISIT_AGE'].mean()\n",
    "            unique_presenting_complaints = active_cases['PRESENTING_COMPLAINT_ENCODED'].nunique()\n",
    "        else:\n",
    "            mean_inverse_ctas = 0\n",
    "            std_inverse_ctas = 0\n",
    "            mean_age = 0\n",
    "            unique_presenting_complaints = 0\n",
    "        \n",
    "        # Append the result to the list\n",
    "        hourly_acuity.append({\n",
    "            'FACILITY_ID': facility,\n",
    "            'Hour': hour,\n",
    "            'Mean_Inverse_CTAS': mean_inverse_ctas,\n",
    "            'Std_Inverse_CTAS': std_inverse_ctas,\n",
    "            'Mean_Age': mean_age,\n",
    "            'Unique_Presenting_Complaints': unique_presenting_complaints\n",
    "        })\n",
    "    \n",
    "   # Convert the list to a DataFrame and append it to the acuity_scores DataFrame\n",
    "    facility_acuity_df = pd.DataFrame(hourly_acuity)\n",
    "    acuity_scores = pd.concat([acuity_scores, facility_acuity_df], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba1282c-9b3b-409a-9594-5157c1232d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display the first few rows of the acuity_scores DataFrame to verify the calculations\n",
    "print(acuity_scores.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597168e1-e629-4ce8-ae4a-d2e12b74fe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_log_visits['Hour'] = df_event_log_visits['Start_Time'].dt.floor('H')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3d8dd5-5322-4c59-bd0e-dfe54058379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_log_visits['Hour'] = df_event_log_visits['Hour'].dt.floor('H')\n",
    "acuity_scores['Hour'] = acuity_scores['Hour'].dt.floor('H')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1740985-0943-4024-bbec-37e9f187ef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Merge the acuity scores back into the original dataframe based on FACILITY_ID and Start_Time\n",
    "merged_df = pd.merge(df_event_log_visits, acuity_scores, how='left', on=['FACILITY_ID', 'Hour'])\n",
    "\n",
    "\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db056e7f-10af-4a86-a74a-1c50eabee686",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed48cae4-bc38-4d36-b825-7978644b4b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the column that is not needed based on my analysis\n",
    "merged_df.drop(columns=['Hour'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4372f50e-a097-4016-9e92-c863f87f42e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9fcea9-f798-449c-b5eb-07af4c5c9acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.dropna(subset=['Std_Inverse_CTAS'])\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeef8047-4ddc-440c-8cbe-2b160cd3b3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_log_visits = merged_df\n",
    "df_event_log_visits.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcec549-273f-4b9b-af31-f83d91ea1057",
   "metadata": {},
   "source": [
    "## IMAGING_DONE and LABS_DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de3ec88-25ea-4f48-b188-700d8e255914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Excel file for imaging data\n",
    "imaging_data = pd.read_excel(\"October2023_Data\\INC10447_ED_VISIT_IMAGES_20230929.xlsx\")\n",
    "\n",
    "# Loading the CSV file for lab tests data\n",
    "labs_data = pd.read_csv(\"October2023_Data\\INC10447_ED_VISIT_LABS_20230929.csv\")\n",
    "\n",
    "# Displaying the first few rows to ensure correct data loading\n",
    "print(\"Imaging Data:\")\n",
    "print(imaging_data.head())\n",
    "print(\"Labs Data:\")\n",
    "print(labs_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96cf8ab-48c3-429b-9792-59f7d7da865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date columns to datetime type\n",
    "imaging_data['ED_SERVICE_DATE'] = pd.to_datetime(imaging_data['ED_SERVICE_DATE'])\n",
    "imaging_data['IMAGE_REPORTED_DATE'] = pd.to_datetime(imaging_data['IMAGE_REPORTED_DATE'])\n",
    "labs_data['ED_SERVICE_DATE'] = pd.to_datetime(labs_data['ED_SERVICE_DATE'])\n",
    "labs_data['LAB_REPORTED_DATE'] = pd.to_datetime(labs_data['LAB_REPORTED_DATE'])\n",
    "\n",
    "# Define the date range\n",
    "start_date = '2022-04-01'\n",
    "end_date = '2023-04-30'\n",
    "\n",
    "# Filter the Imaging Data\n",
    "filtered_imaging_data = imaging_data[\n",
    "    (imaging_data['ED_SERVICE_DATE'] >= start_date) & \n",
    "    (imaging_data['ED_SERVICE_DATE'] <= end_date)\n",
    "]\n",
    "\n",
    "# Filter the Labs Data\n",
    "filtered_labs_data = labs_data[\n",
    "    (labs_data['ED_SERVICE_DATE'] >= start_date) & \n",
    "    (labs_data['ED_SERVICE_DATE'] <= end_date)\n",
    "]\n",
    "\n",
    "# Check the filtered data\n",
    "print(\"Filtered Imaging Data:\", filtered_imaging_data.head())\n",
    "print(\"Filtered Labs Data:\", filtered_labs_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3919bf1f-3cf7-4a00-9382-8a6d8d140602",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_imaging_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa311281-b2a8-47d0-94ea-4978ab17293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_labs_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0e2217-8c11-4eb0-a7b4-c15deade2d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Find unique image test names\n",
    "unique_images = filtered_imaging_data['IMAGE_TEST_NAME'].unique()\n",
    "\n",
    "# Find unique lab test names\n",
    "unique_labs = filtered_labs_data['LAB_TEST_NAME'].unique()\n",
    "\n",
    "print(f\"Unique image test names: {len(unique_images)}\")\n",
    "print(f\"Unique lab test names: {len(unique_labs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bc054c-d519-48ac-a8f2-b29bf6013211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by visit ID and count the number of tests and images\n",
    "image_counts = filtered_imaging_data.groupby('ED_VISIT_ID').size()\n",
    "lab_counts = filtered_labs_data.groupby('ED_VISIT_ID').size()\n",
    "\n",
    "# Calculate min, max, and average for image tests\n",
    "image_min = image_counts.min()\n",
    "image_max = image_counts.max()\n",
    "image_avg = image_counts.mean()\n",
    "\n",
    "# Calculate min, max, and average for lab tests\n",
    "lab_min = lab_counts.min()\n",
    "lab_max = lab_counts.max()\n",
    "lab_avg = lab_counts.mean()\n",
    "\n",
    "print(f\"Image tests - Min: {image_min}, Max: {image_max}, Avg: {image_avg:.2f}\")\n",
    "print(f\"Lab tests - Min: {lab_min}, Max: {lab_max}, Avg: {lab_avg:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b87e0f-66e1-400e-8cf3-22276ecab715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Imaging Data by visit\n",
    "imaging_summary = filtered_imaging_data.groupby(['SID', 'ED_VISIT_ID'])['IMAGE_TEST_NAME'].agg([\n",
    "    ('Imaging_Tests', lambda x: ', '.join(x.unique())),  # Concatenate unique test names\n",
    "    ('Num_Imaging_Tests', 'nunique')                     # Count unique tests\n",
    "]).reset_index()\n",
    "\n",
    "# Aggregate Labs Data by visit\n",
    "labs_summary = filtered_labs_data.groupby(['SID', 'ED_VISIT_ID'])['LAB_TEST_NAME'].agg([\n",
    "    ('Lab_Tests', lambda x: ', '.join(x.unique())),       # Concatenate unique lab names\n",
    "    ('Num_Lab_Tests', 'nunique')                          # Count unique labs\n",
    "]).reset_index()\n",
    "\n",
    "# Merge the summaries with the main dataset\n",
    "df_event_log_visits = df_event_log_visits.merge(imaging_summary, how='left', left_on=['SID', 'VISIT_ID'], right_on=['SID', 'ED_VISIT_ID'])\n",
    "df_event_log_visits = df_event_log_visits.merge(labs_summary, how='left', left_on=['SID', 'VISIT_ID'], right_on=['SID', 'ED_VISIT_ID'])\n",
    "\n",
    "# Clean up the merged DataFrame\n",
    "df_event_log_visits.drop(columns=['ED_VISIT_ID_x', 'ED_VISIT_ID_y'], inplace=True)\n",
    "\n",
    "# Replace NaNs in new columns with appropriate defaults\n",
    "df_event_log_visits[['Imaging_Tests', 'Lab_Tests']].fillna('None', inplace=True)\n",
    "df_event_log_visits[['Num_Imaging_Tests', 'Num_Lab_Tests']].fillna(0, inplace=True)\n",
    "\n",
    "# Display to verify\n",
    "print(df_event_log_visits.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc609db-73c5-4531-98c9-61e2a8fd6625",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_log_visits.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0051a8-a9ba-4305-b8ed-44719190c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check in the Imaging Data\n",
    "imaging_presence = filtered_imaging_data[filtered_imaging_data['ED_VISIT_ID'] == 'VISIT2354278']\n",
    "\n",
    "# Check in the Labs Data\n",
    "labs_presence = filtered_labs_data[filtered_labs_data['ED_VISIT_ID'] == 'VISIT2354278']\n",
    "\n",
    "# Print the results\n",
    "print(\"Imaging Data:\")\n",
    "print(imaging_presence)\n",
    "print(\"\\nLabs Data:\")\n",
    "print(labs_presence)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4cba49-70eb-435f-a011-319de222d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check in the Imaging Data\n",
    "imaging_presence = filtered_imaging_data[filtered_imaging_data['ED_VISIT_ID'] == 'VISIT2234433']\n",
    "\n",
    "# Check in the Labs Data\n",
    "labs_presence = filtered_labs_data[filtered_labs_data['ED_VISIT_ID'] == 'VISIT2234433']\n",
    "\n",
    "# Print the results\n",
    "print(\"Imaging Data:\")\n",
    "print(imaging_presence)\n",
    "print(\"\\nLabs Data:\")\n",
    "print(labs_presence)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94c238f-a8ee-4319-8de4-55c39f75376d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for the specific VISIT_ID\n",
    "filtered_data = df_event_log_visits[df_event_log_visits['VISIT_ID'] == 'VISIT2234433']\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "filtered_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996b892c-8c42-4da5-aca8-95501772acb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values with 0 in the 'Num_Imaging_Tests' and 'Num_Lab_Tests' columns\n",
    "df_event_log_visits['Num_Imaging_Tests'].fillna(0, inplace=True)\n",
    "df_event_log_visits['Num_Lab_Tests'].fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6930c517-1e10-49af-b083-015537c5e1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_log_visits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac91781-1695-41ca-a782-6e0cbf1c3b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_log_visits.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85be0ca1-05a0-4198-8fe0-b2d07915d306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the original columns\n",
    "df_event_log_visits = df_event_log_visits.drop(columns=['Imaging_Tests', 'Lab_Tests'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520ead7c-417d-4149-9693-665d27799aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_log_visits.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8b5ee5-92db-4fa7-bd27-96016d9ee705",
   "metadata": {},
   "source": [
    "## Number of test/img on daily bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6714c36-c523-4ea6-aafb-ae47fe5463a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_log_visits.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab11383-0fb6-4824-9460-84e0cff5417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'Start_Time' is datetime if it's not already\n",
    "df_event_log_visits['Start_Time'] = pd.to_datetime(df_event_log_visits['Start_Time'])\n",
    "\n",
    "# Extract the date from 'Start_Time' to facilitate grouping\n",
    "df_event_log_visits['Date'] = df_event_log_visits['Start_Time'].dt.date\n",
    "\n",
    "# Convert 'Date' to datetime in df_event_log_visits\n",
    "df_event_log_visits['Date'] = pd.to_datetime(df_event_log_visits['Date'])\n",
    "\n",
    "# Group by 'FACILITY_ID' and 'Date' to calculate the number of imaging tests and lab tests\n",
    "daily_tests = df_event_log_visits.groupby(['FACILITY_ID', 'Date']).agg(\n",
    "    Daily_Imaging_Tests=('Num_Imaging_Tests', 'sum'),\n",
    "    Daily_Lab_Tests=('Num_Lab_Tests', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Verify the calculated statistics\n",
    "print(daily_tests.head())\n",
    "\n",
    "# Convert 'Date' to datetime in daily_tests\n",
    "daily_tests['Date'] = pd.to_datetime(daily_tests['Date'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa24107-3e08-418f-ab3e-f27347077bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original dataset if not already loaded\n",
    "# df_event_log_visits = pd.read_csv('your_data.csv')  # Uncomment and modify this line to load your actual data\n",
    "\n",
    "# Ensure 'Start_Time' is datetime if it's not already\n",
    "df_event_log_visits['Start_Time'] = pd.to_datetime(df_event_log_visits['Start_Time'])\n",
    "\n",
    "# Extract the date from 'Start_Time' to facilitate merging\n",
    "df_event_log_visits['Date'] = df_event_log_visits['Start_Time'].dt.date\n",
    "\n",
    "# Convert 'Date' to datetime in df_event_log_visits\n",
    "df_event_log_visits['Date'] = pd.to_datetime(df_event_log_visits['Date'])\n",
    "\n",
    "# Convert 'Date' to datetime if it's not already\n",
    "daily_tests['Date'] = pd.to_datetime(daily_tests['Date'])\n",
    "\n",
    "# Merge the daily test counts back into the original dataframe based on FACILITY_ID and Date\n",
    "df_event_log_visits = pd.merge(df_event_log_visits, daily_tests, on=['FACILITY_ID', 'Date'], how='left')\n",
    "\n",
    "# Drop the 'Date' column from the merge if it's no longer needed\n",
    "df_event_log_visits = df_event_log_visits.drop(columns=['Date'])\n",
    "\n",
    "df_event_log_visits.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae7cb97-89c8-45ed-bb89-b6d38bb65c0c",
   "metadata": {},
   "source": [
    "## Deleting columns that were created during the creation of new features and are not needed anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c86b5e2-a844-4a1d-8187-576555b3b2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to drop\n",
    "columns_to_drop = ['FACILITY_NAME',\n",
    "    'Country', \n",
    "    'Postal_Code', \n",
    "    'Place_Name', \n",
    "    'Province', \n",
    "    'Province_Code', \n",
    "    'First_3_Digits',  \n",
    "    'DOB', 'DOD', 'Latitude', 'Longitude', 'ED_POSTAL_CODE'\n",
    "]\n",
    "\n",
    "# Dropping the specified columns\n",
    "df_event_log_visits.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Displaying the updated DataFrame to confirm the changes\n",
    "print(df_event_log_visits.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7e0c18-3b93-43fa-a6b5-454c404cf1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_log_visits.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06e6a1c-1c49-413a-8928-7c11fd15280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding for 'FACILITY_ID'\n",
    "df_event_log_visits = pd.get_dummies(df_event_log_visits, columns=['FACILITY_ID'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251a25b3-62d6-4eb0-959d-4202852b6e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Applying one-hot encoding to the 'Activity' column\n",
    "dummies = pd.get_dummies(df_event_log_visits['Activity'], prefix='Activity')\n",
    "\n",
    "# Concatenating the original DataFrame with the new dummy variables DataFrame\n",
    "df_event_log_visits = pd.concat([df_event_log_visits, dummies], axis=1)\n",
    "\n",
    "# Checking the updated DataFrame\n",
    "print(df_event_log_visits.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b8a7b2-219f-499b-ad79-a997329e448a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a binary outcome for 'DEPART_DISPOSITION'\n",
    "df_event_log_visits['Is_LWBS'] = df_event_log_visits['DEPART_DISPOSITION_ID'].isin(['TLWBS', 'RLWBS']).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061147e5-5db0-47d7-affe-44b3c43b010a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_log_visits.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf84f30e-2ddf-4a16-b976-1e676166725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping original columns after encoding\n",
    "columns_to_drop = ['Activity']\n",
    "\n",
    "df_event_log_visits.drop(columns=columns_to_drop, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda45965-26bd-4cb8-86ca-3f8899effaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting 'End_Time' to datetime\n",
    "df_event_log_visits['Activity_End_Time'] = pd.to_datetime(df_event_log_visits['Activity_End_Time'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b2315a-096c-41cb-8fd2-9711cc27deeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Defining the order for 'TIME_OF_THE_DAY'\n",
    "time_order = [['Early Morning to Morning', 'Late Morning to Late Afternoon','Evening to Night']]\n",
    "\n",
    "# Creating an OrdinalEncoder with the specified order\n",
    "ordinal_encoder = OrdinalEncoder(categories=time_order)\n",
    "\n",
    "# Applying the encoder to 'TIME_OF_THE_DAY'\n",
    "df_event_log_visits['TIME_OF_THE_DAY_Ordinal'] = ordinal_encoder.fit_transform(df_event_log_visits[['TIME_OF_THE_DAY']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c92ace3-45bd-4bad-a17b-8664aff0b8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['TIME_OF_THE_DAY']\n",
    "\n",
    "df_event_log_visits.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece53249-d379-490b-828c-67cfd8b1f82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the order for 'Visit_Season'\n",
    "season_order = [['Winter', 'Spring', 'Summer', 'Fall']]\n",
    "\n",
    "# Creating an OrdinalEncoder with the specified order\n",
    "ordinal_encoder = OrdinalEncoder(categories=season_order)\n",
    "\n",
    "# Applying the encoder to 'Visit_Season'\n",
    "df_event_log_visits['Visit_Season_Ordinal'] = ordinal_encoder.fit_transform(df_event_log_visits[['Visit_Season']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a450ac7-ab83-4ed7-8c84-63b8b13c8465",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['Visit_Season']\n",
    "\n",
    "df_event_log_visits.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e8c953-1289-4360-8658-0cc2f80597e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_log_visits.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cd4cdf-5c5d-4e2b-87a0-1e69a21fe10c",
   "metadata": {},
   "source": [
    "## changes discussed at todays meeting 2024/04/29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01705f4-96c5-405b-a733-6965f1bce02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changes discussed at todays meeting 2024/04/29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0763500-e507-4936-b262-b996b66cc9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the duration of each activity in hours\n",
    "df_event_log_visits['Start_Time'] = pd.to_datetime(df_event_log_visits['Start_Time'])\n",
    "df_event_log_visits['Activity_End_Time'] = pd.to_datetime(df_event_log_visits['Activity_End_Time'])\n",
    "\n",
    "df_event_log_visits['Activity_Duration'] = (df_event_log_visits['Activity_End_Time'] - df_event_log_visits['Start_Time']).dt.total_seconds() / 3600\n",
    "print(df_event_log_visits.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75aa441-7fea-4f13-bb54-7076d89ecee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows with the correct column names\n",
    "df_event_log_visits[['VISIT_ID', 'Start_Time', 'Activity_End_Time', 'Activity_Duration']].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649153a8-fd90-455b-b7c4-475f232fae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_log_visits.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251a2d8b-c9e7-4364-9bd9-6f772c95f7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping start and end times as they are no longer needed\n",
    "columns_to_drop = ['Start_Time', 'Activity_End_Time']\n",
    "\n",
    "df_event_log_visits.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Renaming the 'Duration_hours' column to 'Case_Duration_Hours'\n",
    "df_event_log_visits.rename(columns={'Duration_hours': 'Case_Duration_Hours'}, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfff374e-0a8e-4ed3-b0e9-9bf6e73fdede",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_event_log_visits.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545c7d00-adbb-45e2-8d5d-7f1f4ddb3854",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_log_visits.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e387d1-8bc2-4caa-9e0d-a51afa6f0001",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_log_visits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a70b020-df4d-499d-8633-6b6c12400413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'DEPART_DISPOSITION_ID' is missing\n",
    "df_event_log_visits.dropna(subset=['DEPART_DISPOSITION_ID'], inplace=True)\n",
    "\n",
    "# Display the DataFrame to verify that the rows have been removed\n",
    "print(df_event_log_visits[['SID', 'VISIT_ID', 'DEPART_DISPOSITION_ID']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8f803d-0955-43ec-9d3f-2734c4cb5731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting occurrences of each 'DEPART_DISPOSITION_ID' within each 'SID'\n",
    "disposition_counts = df_event_log_visits.groupby(['SID', 'DEPART_DISPOSITION_ID']).size()\n",
    "\n",
    "# Converting the series to a DataFrame\n",
    "disposition_counts = disposition_counts.reset_index(name='Counts')\n",
    "\n",
    "# Calculating total occurrences of dispositions per 'SID'\n",
    "total_dispositions_per_sid = disposition_counts.groupby('SID')['Counts'].transform('sum')\n",
    "\n",
    "# Calculating frequency of each disposition within each SID\n",
    "disposition_counts['Disposition_Frequency'] = disposition_counts['Counts'] / total_dispositions_per_sid\n",
    "\n",
    "# Merging this frequency back to the original DataFrame\n",
    "df_event_log_visits = df_event_log_visits.merge(disposition_counts[['SID', 'DEPART_DISPOSITION_ID', 'Disposition_Frequency']], on=['SID', 'DEPART_DISPOSITION_ID'], how='left')\n",
    "\n",
    "# Displaying the first few rows to verify the changes\n",
    "df_event_log_visits[['SID', 'DEPART_DISPOSITION_ID', 'Disposition_Frequency']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd38ef8-47cb-4776-94f4-062d2086340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['DEPART_DISPOSITION_ID']\n",
    "\n",
    "df_event_log_visits.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb2cf5-10bc-4cf5-9ef3-1ed8d8687487",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event_log_visits.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7fd681-d7ea-4522-bd3d-5ceeba19c37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "df_event_log_visits.to_csv('df_event_log_visits_for_ML_randomorder_areafix.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb79916-c59f-491a-82d1-3fbfae2d38cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
